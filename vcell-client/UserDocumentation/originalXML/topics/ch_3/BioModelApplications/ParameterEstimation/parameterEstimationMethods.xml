<?xml version="1.0" encoding="UTF-8"?>
<vcelldoc>
<page title="COPASI Optimization Methods Overview">

	<introduction>
	<para>   
	COPASI parameter estimation provides tools to optimize parameters in non-spatial deterministic models to
	best fit experimental data. Parameter Estimation is run within the Virtual Cell on your own desktop
	rather than on our central database servers. Once you have the results of a parameter estimation, you
	will save the model parameters with the optimized parameter set as a newly created simulation. Thus,
	the results will be saved permanently as part of this Virtual Cell model.
	</para>
	<para>
	COPASI parameter estimation has provided the following optmization solvers. Brief descriptions for different
	solvers are shown below. For more detailed decription, please visit  <link target = "http://www.copasi.org/tiki-view_articles.php"> COPASI website </link>.
	</para>
	<list>
	<item>Evolutionary Programming</item>
	<para>
	Evolutionary programming (EP) is a computational technique that mimics evolution and is based on 
	reproduction and selection. An EP algorithm is composed of individuals that reproduce and compete, 
	each one is a potential solution to the (optimization) problem and is represented by a 'genome' where
	each gene corresponds to one adjustable parameter. At each generation of the EP, each individual 
	reproduces asexually, i.e. divides into two individuals. One of these contains exactly the same 'genome'
	as the parent while the other suffers some mutations(the parameter values of each gene change slightly).
	At the end of the generation, the algorithm has double the number of individuals. Then each of the
	individuals is confronted with a number of others to count how many does it outperform (the number of
	wins is the number of these competitors that represent worse solutions than itself).All the individuals
	are ranked by their number of wins, and the population is again reduced to the original number of individuals
	by eliminating those which have worse fitness (solutions).
	</para>
	<para></para>
	<item>Evolution Strategy (SRES)</item>
	<para>
	Evolutionary Strategies with Stochastic Ranking (SRES) [Runarsson00] is similar to . However, a parent
	has multiple offsprings during each generation. Each offspring will contain a recombination of genes with
	another parent and additional mutations. The algorithm assures that each parameter value will be within its
	boundaries. But constraints to the solutions may be violated.
	</para>
	<para></para>
	<item>Genetic Algorithm</item>
	<para>
	The genetic algorithm (GA) [Baeck97 Baeck93 Michalewicz94 Mitchell95] is a computational technique that mimics
	evolution and is based on reproduction and selection. A GA is composed of individuals that reproduce and compete, 
	each one is a potential solution to the (optimization) problem and is represented by a 'genome' where each
	gene corresponds to one adjustable parameter. At each generation of the GA, each individual is paired with one
	other at random for reproduction.Two offspring are produced by combining their genomes and allowing for 'cross-over',
	i.e., the two new individuals have genomes that are formed from a combination of the genomes of their parents.
	Also each new gene might have mutated, i.e.the parameter value might have changed slightly. At the end of the generation,
	the algorithm has double the number of individuals. Then each of the individuals is confronted with a number of
	others to count how many does it outperform (the number of wins is the number of these competitors that represent
	worse solutions than itself). All the individuals are ranked by their number of wins, and the population is again
	reduced to the original number of individuals by eliminating those which have worse fitness (solutions).
	</para>
	<para></para>
	<item>Genetic Algorithm SR</item>
	<para>
	The genetic algorithm with stochastic ranking is very similar to the before described with tournament selection.
	With two exception which are the mutations are not forced to be within the boundaries and the selection is done
	through a bubble sort with a random factor
	</para>
	<para></para>
	<item>Hooke and Jeeves</item>
	<para>
	The method of Hooke and Jeeves is a direct search algorithm that searches for the minimum of a nonlinear function
	without requiring (or attempting to calculate) derivatives of the function. Instead it is based on a heuristic
	that suggests a descent direction using the values of the function calculated in a number of previous iterations.
	</para>
	<para></para>
	<item>Levenberg - Marquardt</item>
	<para>
	Levenberg-Marquardt is a gradient descent method. It is a hybrid between the steepest descent and the Newton methods.
	</para>
	<para></para>
	<item>Nelder - Mead</item>
	<para>
	This method also known as the simplex method is due to Nelder and Mead [Nelder65]. A simplex is a polytope of N+1 vertices
	in N dimensions. The objective function is evaluated at each vertex. Dependent on these calculated values a new simplex
	is constructed. The simplest step is to replace the worst point with a point reflected through the centroid of the
	remaining N points.If this point is better than the best current point, then we can try stretching exponentially out
	along this line. On the other hand,if this new point isn't much better than the previous value then we are stepping
	across a valley, so we shrink the simplex towards the best point.
	</para>
	<para></para>
	<item>Particle Swarm</item>
	<para>
	The particle swarm optimization method suggested by Kennedy and Eberhart [Kennedy95] is inspired by a flock of birds
	or a school of fish searching for food. Each particle has a position Xi and a velocity Vi in the parameter space.
	Additionally, it remembers its best achieved objective value O and position Mi. Dependent on its own information
	and the position of its best neighbor (a random subset of particles of the swarm) a new velocity is calculated. With
	this information the position is updated.
	</para>
	<para></para>
	<item>Random Search</item>
	<para>
	Random search is an optimization method that attempts to find the optimum by testing the objective function's
	value on a series of combinations of random values of the adjustable parameters. The random values are generated
	complying with any boundaries selected by the user, furthermore, any combinations of parameter values that do not
	fulfill constraints on the variables are excluded. This means that the method is capable of handling bounds on
	the adjustable parameters and fulfilling constraints. For infinite number of iterations this method is guaranteed
	to find the global optimum of the objective function. In general one is interested in processing a very large
	number of iterations.
	</para>
	<para></para>
	<item>Simulated Annealing</item>
	<para>
	Simulated annealing is an optimization algorithm first proposed by Kirkpatrick et al. [Kirkpatrick83] and was
	inspired by statistical mechanics and the way in which perfect crystals are formed. Perfect crystals are formed
	by first melting the substance of interest, and then cooling it very slowly. At large temperatures the particles
	vibrate with wide amplitude and this allows a search for global optimum.As the temperature decreases so do the
	vibrations until the system settles to the global optimum (the perfect crystal). The simulated annealing
	optimization algorithm uses a similar concept: the objective function is considered a measure of the energy of 
	the system and this is maintained constant for a certain number of iterations (a temperature cycle). In each iteration,
	the parameters are changed to a nearby location in parameter space and the new objective function value calculated;
	if it decreased, then the new state is accepted, if it increased then the new state is accepted with a probability
	that follows a Boltzmann distribution (higher temperature means higher probability of accepting the new state).
	After a fixed number of iterations, the stopping criterion is checked; if it is not time to stop, then the
	system's temperature is reduced and the algorithm continues.
	</para>
	<para></para>
	<item>Steepest Descent</item>
	<para>
	Steepest descent is an optimization method that follows the direction of steepest descent on the hyper-surface
	of the objective function to find a local minimum. The direction of steepest descent is defined by the negative
	of the gradient of the objective function.
	</para>
	<para></para>
	<item>Praxis</item>
	<para>
	Praxis is a direct search method (for a review see [Swann72]) that searches for the minimum of a nonlinear
	function without requiring(or attempting to calculate) derivatives of that function. Praxis was developed by Brent
	after the method proposed by Powell. The inspiration for Praxis was the well-known method of minimising each
	adjustable parameter (direction) at a time - the principal axes method.In Praxis directions are chosen that do not
	coincide with the principal axes, in fact if the objective function is quadratic then these will be conjugate
	directions, assuring a fast convergence rate.
	</para>
	<para></para>
	<item>Truncated Newton</item>
	<para>
	 The Truncated Newton method is a sophisticated variant of the Newton optimization method. The Newton optimization
	 method searches for theminimum of a nonlinear function by following descent directions determined from the function's
	 first and second partial derivatives. TheTruncated Newton method does an incomplete (truncated) solution of a system
	 of linear equations to calculate the Newton direction. This meansthat the actual direction chosen for the descent
	 is between the steepest descent direction and the true Newton direction.
	</para>
	</list>
	
	</introduction>
	
</page>

</vcelldoc>