#!/bin/bash --login
#SBATCH --job-name=advancedJob
#SBATCH --nodes=1
#SBATCH --ntasks=3
#SBATCH --qos=vcell
#SBATCH --partition=vcell
#SBATCH -o %x.stdout
#SBATCH -e %x.stderr
#SBATCH --cpus-per-task=1
#SBATCH --time=01:00:00

log_file="job_status.log"
total_jobs=7  # Total number of jobs to run
timeout_duration=10s  # Maximum allowed runtime for each job
max_concurrent_jobs=$SLURM_NTASKS  # Number of jobs allowed at once

# Clear the log file at the start
echo "Job Execution Log" > $log_file
echo "------------------" >> $log_file

job_pids=()  # Array to track job PIDs
running_jobs=0

for i in $(seq 1 $total_jobs); do
    # Log job start
    echo "Job $i started at $(date)" >> $log_file
    timeout $timeout_duration srun -N 1 -n 1 -c 1 ./subscript_$i.sh &  # Run each script in parallel
    job_pids+=($!)  # Store the PID
    ((running_jobs++))  # Increment running job count

    # Wait for a finished job before launching a new one if we hit the concurrency limit
    while (( running_jobs >= max_concurrent_jobs )); do
        for idx in "${!job_pids[@]}"; do
            if ! kill -0 "${job_pids[$idx]}" 2>/dev/null; then  # Check if process is still running
                wait "${job_pids[$idx]}"  # Ensure exit status is collected
                exit_code=$?
                echo "Job with PID ${job_pids[$idx]} finished with exit code $exit_code at $(date)" >> $log_file
                unset "job_pids[$idx]"  # Remove from tracking
                ((running_jobs--))  # Decrement count
                break  # Break once we free up a slot
            fi
        done
        sleep 1  # Allow brief pause before rechecking
    done
done

# Final wait for any remaining jobs
for pid in "${job_pids[@]}"; do
    wait $pid
    exit_code=$?
    echo "Job with PID $pid finished with exit code $exit_code at $(date)" >> $log_file
done

echo "All jobs completed at $(date)" >> $log_file
echo "End Script" >> $log_file
