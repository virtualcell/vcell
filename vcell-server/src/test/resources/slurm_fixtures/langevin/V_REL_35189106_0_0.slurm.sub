#!/usr/bin/bash
#SBATCH --partition=vcell
#SBATCH --reservation=
#SBATCH --qos=vcell
#SBATCH -J V_REL_35189106_0_0
#SBATCH -o /share/apps/vcell3/htclogs/V_REL_35189106_0_0.slurm.log
#SBATCH -e /share/apps/vcell3/htclogs/V_REL_35189106_0_0.slurm.log
#SBATCH --ntasks=20
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=2048M
#SBATCH --nodes=1
#SBATCH --time=24:00:00
#SBATCH --no-kill
#SBATCH --no-requeue

set -o errexit
set -o pipefail
set -o nounset

# ---- injected configuration (exact locations / naming conventions) ----
SIM_ID=35189106
USERID=danv
JOB_NAME=V_REL_${SIM_ID}_0_0
HTC_LOG_DIR_EXTERNAL=/share/apps/vcell3/htclogs
PRIMARY_DATA_DIR_EXTERNAL=/share/apps/vcell3/users/danv
SECONDARY_DATA_DIR_EXTERNAL=/share/apps/vcell7/users
SIMARCHIVE_HOST=   # keep generator-provided value if any
SLURM_TMPDIR=/scratch/vcell
SINGULARITY_CACHEDIR=/share/apps/vcell3/singularity/cachdir
SINGULARITY_PULLFOLDER=/share/apps/vcell3/singularity/pullfolder
SINGULARITY_MODULE_NAME=singularity/vcell-3.10.0

# Docker images from generator (use configured values)
SOLVER_DOCKER_NAME=ghcr.io/virtualcell/vcell-batch:7.6.0.43
BATCH_DOCKER_NAME=ghcr.io/virtualcell/vcell-batch:7.6.0.43

# derived paths
SLURM_SUB_PATH="${HTC_LOG_DIR_EXTERNAL}/${JOB_NAME}.slurm.sub"
SLURM_LOG_PATH="${HTC_LOG_DIR_EXTERNAL}/${JOB_NAME}.slurm.log"

# runtime parameters (replace TOTAL_JOBS and TIMEOUT_SECONDS with generator values at runtime)
TOTAL_JOBS=10
TIMEOUT_SECONDS=300
MAX_CONCURRENT_JOBS=${SLURM_NTASKS:-20}   # uses SBATCH --ntasks

# create required dirs if missing
mkdir -p "${HTC_LOG_DIR_EXTERNAL}"
mkdir -p "${PRIMARY_DATA_DIR_EXTERNAL}"
mkdir -p "${SLURM_TMPDIR}"

# ---- environment / singularity setup (preserve production ordering) ----
echo "$(hostname)"
export MODULEPATH=/isg/shared/modulefiles:/tgcapps/modulefiles
source /usr/share/Modules/init/bash
module load "${SINGULARITY_MODULE_NAME}"
export SINGULARITY_CACHEDIR="${SINGULARITY_CACHEDIR}"
export SINGULARITY_PULLFOLDER="${SINGULARITY_PULLFOLDER}"
echo "job running on host $(hostname -f)"
echo "id is $(id)"
echo "ENVIRONMENT"
env

# container bindings in production order (host -> container)
container_bindings="--bind ${PRIMARY_DATA_DIR_EXTERNAL}:/simdata "
container_bindings+="--bind ${SECONDARY_DATA_DIR_EXTERNAL}:/simdata_secondary "
if [[ -n "${SIMARCHIVE_HOST}" ]]; then
  container_bindings+="--bind ${SIMARCHIVE_HOST}:/archive "
fi
container_bindings+="--bind ${HTC_LOG_DIR_EXTERNAL}:/htclogs "
container_bindings+="--bind ${SLURM_TMPDIR}:/solvertmp "

# compute java_mem_Xmx from mem-per-task; mem-per-cpu is 2048M, cpus-per-task=1 -> mem/task=2048M
# subtract small overhead; adjust formula as needed by generator
JAVA_MEM_XMX=1800M

container_env="--env java_mem_Xmx=${JAVA_MEM_XMX} "
container_env+="--env jmshost_sim_internal=rke-wn-01.cam.uchc.edu "
container_env+="--env jmsport_sim_internal=31618 "
container_env+="--env jmsrestport_sim_internal=30163 "
container_env+="--env jmsuser=clientUser "
container_env+="--env jmspswd=dummy "
container_env+="--env jmsblob_minsize=100000 "
container_env+="--env mongodbhost_internal=rke-wn-01.cam.uchc.edu "
container_env+="--env mongodbport_internal=30019 "
container_env+="--env mongodb_database=test "
container_env+="--env primary_datadir_external=${PRIMARY_DATA_DIR_EXTERNAL} "
container_env+="--env secondary_datadir_external=${SECONDARY_DATA_DIR_EXTERNAL} "
container_env+="--env htclogdir_external=${HTC_LOG_DIR_EXTERNAL} "
container_env+="--env softwareVersion=Rel_Version_7.6.0_build_28 "
container_env+="--env serverid=REL "

solver_container_prefix="singularity run --containall ${container_bindings} ${container_env} docker://${SOLVER_DOCKER_NAME}"
batch_container_prefix="singularity run --containall ${container_bindings} ${container_env} docker://${BATCH_DOCKER_NAME}"

# helper: send failure message via batch container (preserve production call)
sendFailureMsg() {
  ${batch_container_prefix} --msg-userid clientUser --msg-password dummy --msg-host rke-wn-01.cam.uchc.edu --msg-port 31618 --msg-job-host "$(hostname)" --msg-job-userid "${USERID}" --msg-job-simkey "${SIM_ID}" --msg-job-jobindex 0 --msg-job-taskid 0 --msg-job-errmsg "$1" SendErrorMsg
  stat=$?
  if [[ $stat -ne 0 ]]; then
    echo "failed to send error message, retcode=$stat"
  else
    echo "sent failure message"
  fi
}

# exit processor wrapper calling JavaPostprocessor64 inside batch container
callExitProcessor() {
  # usage: callExitProcessor <exitCode>
  ${batch_container_prefix} JavaPostprocessor64 "${SIM_ID}" "${USERID}" 17 0 0 "$1" "${SLURM_SUB_PATH}"
  return $?
}

# central logging for this script
LOG_FILE="${HTC_LOG_DIR_EXTERNAL}/${JOB_NAME}_${SLURM_JOB_ID}_submit.log"
echo "Job Execution Log" > "${LOG_FILE}"
echo "------------------" >> "${LOG_FILE}"
echo "=== [$(date)] === SLURM job ${SLURM_JOB_ID} started on $(hostname)" | tee -a "${LOG_FILE}"

# ---- Preprocess (must succeed or call exit processor and exit) ----
SIMXML="/simdata/SimID_${SIM_ID}_0__0.simtask.xml"
echo "[Preprocess] Running JavaPreprocessor64 ${SIMXML}" | tee -a "${LOG_FILE}"
${batch_container_prefix} JavaPreprocessor64 "${SIMXML}" /simdata >> "${LOG_FILE}" 2>&1
stat=$?
echo "JavaPreprocessor64 returned $stat" | tee -a "${LOG_FILE}"
if [ $stat -ne 0 ]; then
  callExitProcessor $stat
  echo "Preprocessor failed; exiting ${stat}" | tee -a "${LOG_FILE}"
  exit $stat
fi

# ---- parallel simulation launch with concurrency throttling (preserve your logic) ----
INPUT_DIR="/simdata"
OUTPUT_DIR="/simdata"   # solver writes results to user dir; solver logs go to /htclogs via --output-log
LOG_DIR="/htclogs"
SLURM_JOB_NAME="${JOB_NAME}"

declare -A job_pid_map
job_pids=()
running_jobs=0
any_fail=0

for i in $(seq 0 $((TOTAL_JOBS - 1))); do
    echo "Job $i started at $(date)" | tee -a "${LOG_FILE}"
    timeout "${TIMEOUT_SECONDS}s" srun -N 1 -n 1 -c 1 \
       singularity exec --cleanenv \
         --bind "${PRIMARY_DATA_DIR_EXTERNAL}:${PRIMARY_DATA_DIR_EXTERNAL}" \
         --bind "${HTC_LOG_DIR_EXTERNAL}:${HTC_LOG_DIR_EXTERNAL}" \
         --bind "${SLURM_TMPDIR}:${SLURM_TMPDIR}" \
         docker://${SOLVER_DOCKER_NAME} \
         langevin_x64 simulate \
           "${INPUT_DIR}/SimID_${SIM_ID}_0_.langevinInput" \
           "${i}" \
           --output-log="${LOG_DIR}/${SLURM_JOB_NAME}_${i}.log" \
           --vc-print-status &
    pid=$!
    job_pids+=($pid)
    job_pid_map[$pid]=$i
    ((running_jobs++))

    # throttle: wait for any finished job before launching new ones when concurrency limit hit
    while (( running_jobs >= MAX_CONCURRENT_JOBS )); do
        for idx in "${!job_pids[@]}"; do
            pid="${job_pids[$idx]}"
            if [[ -z "${pid}" ]]; then
                continue
            fi
            if ! kill -0 "$pid" 2>/dev/null; then
                wait "$pid"
                exit_code=$?
                job_index=${job_pid_map[$pid]:-unknown}
                echo "Job $job_index with PID $pid finished with exit code $exit_code at $(date)" | tee -a "${LOG_FILE}"
                unset "job_pids[$idx]"
                unset "job_pid_map[$pid]"
                ((running_jobs--))
                if [[ $exit_code -ne 0 ]]; then
                    any_fail=1
                fi
                break
            fi
        done
        sleep 1
    done
done

# final wait for any remaining jobs
for pid in "${job_pids[@]}"; do
    if [[ -z "${pid}" ]]; then
        continue
    fi
    wait "$pid"
    exit_code=$?
    job_index=${job_pid_map[$pid]:-unknown}
    echo "Job $job_index with PID $pid finished with exit code $exit_code at $(date)" | tee -a "${LOG_FILE}"
    if [[ $exit_code -ne 0 ]]; then
        any_fail=1
    fi
done

echo "Batch jobs completed at $(date)" | tee -a "${LOG_FILE}"

# ---- start the postprocess solver invocation (must run after all simulation jobs finished) ----
echo "Starting the last (postprocess) job at $(date)" | tee -a "${LOG_FILE}"
timeout "${TIMEOUT_SECONDS}s" srun -N 1 -n 1 -c 1 \
       singularity exec --cleanenv \
         --bind "${PRIMARY_DATA_DIR_EXTERNAL}:${PRIMARY_DATA_DIR_EXTERNAL}" \
         --bind "${HTC_LOG_DIR_EXTERNAL}:${HTC_LOG_DIR_EXTERNAL}" \
         --bind "${SLURM_TMPDIR}:${SLURM_TMPDIR}" \
         docker://${SOLVER_DOCKER_NAME} \
      langevin_x64 postprocess \
        "${INPUT_DIR}/SimID_${SIM_ID}_0_.langevinInput" \
        "${TOTAL_JOBS}" \
        --output-log="${LOG_DIR}/${SLURM_JOB_NAME}_P.log" \
        --vc-print-status &
last_pid=$!
wait $last_pid
exit_code=$?
echo "Job 'Last' with PID $last_pid finished with exit code $exit_code at $(date)" | tee -a "${LOG_FILE}"
echo "The final job finished at $(date)" | tee -a "${LOG_FILE}"
echo "All jobs completed at $(date)" | tee -a "${LOG_FILE}"

# ---- run JavaPostprocessor64 inside batch container and exit with appropriate code ----
echo "[Postprocess] Running JavaPostprocessor64..." | tee -a "${LOG_FILE}"
${batch_container_prefix} JavaPostprocessor64 "${SIM_ID}" "${USERID}" 17 0 0 "${TOTAL_JOBS}" "${SLURM_SUB_PATH}" >> "${LOG_FILE}" 2>&1
post_exit=$?

# decide final exit code: prefer any_fail or post_exit if non-zero
if [[ "${any_fail}" -ne 0 ]]; then
  echo "One or more simulation tasks failed; exiting non-zero" | tee -a "${LOG_FILE}"
  exit 1
fi

if [[ "${post_exit}" -ne 0 ]]; then
  echo "JavaPostprocessor64 failed with ${post_exit}; exiting ${post_exit}" | tee -a "${LOG_FILE}"
  exit "${post_exit}"
fi

echo "=== [$(date)] === SLURM job completed successfully" | tee -a "${LOG_FILE}"
exit 0
